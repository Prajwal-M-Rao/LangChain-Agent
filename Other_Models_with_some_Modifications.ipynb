{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prajwal-M-Rao/LangChain-Agent/blob/main/Other_Models_with_some_Modifications.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYnyMsonOC_U"
      },
      "outputs": [],
      "source": [
        "pip install langchain langchain-community langchain-openai ollama wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvmJWx-lNpVj",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "from transformers import pipeline\n",
        "\n",
        "# Set up a local pipeline using a small model that can run on CPU\n",
        "# This doesn't require API keys or external services\n",
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",  # A small model that can run on CPU\n",
        "    max_new_tokens=150,\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "# Create a LangChain wrapper around the pipeline\n",
        "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
        "\n",
        "# Set up the Wikipedia tool\n",
        "wikipedia = WikipediaAPIWrapper(\n",
        "    top_k_results=2,\n",
        "    doc_content_chars_max=4000\n",
        ")\n",
        "wikipedia_tool = WikipediaQueryRun(api_wrapper=wikipedia)\n",
        "\n",
        "# Define the prompt template for our agent\n",
        "prompt_template = \"\"\"You are a helpful assistant that answers user questions using your knowledge and Wikipedia when necessary.\n",
        "You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original user question\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {input}\n",
        "{agent_scratchpad}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "# Create the agent\n",
        "agent = create_react_agent(\n",
        "    llm=llm,\n",
        "    tools=[wikipedia_tool],\n",
        "    prompt=prompt\n",
        ")\n",
        "\n",
        "# Create the agent executor\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=[wikipedia_tool],\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True\n",
        ")\n",
        "\n",
        "def process_query(user_query):\n",
        "    \"\"\"Process a natural language query through the agent.\"\"\"\n",
        "    try:\n",
        "        result = agent_executor.invoke({\"input\": user_query})\n",
        "        return result[\"output\"]\n",
        "    except Exception as e:\n",
        "        return f\"Error processing your query: {str(e)}\"\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Welcome to the LangChain Wikipedia Agent!\")\n",
        "    print(\"Ask any question, and I'll use my knowledge and Wikipedia to answer.\")\n",
        "    print(\"Type 'exit' to quit.\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nYour question: \")\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        print(\"\\nProcessing your query...\")\n",
        "        response = process_query(user_input)\n",
        "        print(f\"\\nResponse: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain openai tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QHxlW4kBZNt",
        "outputId": "2a54e30c-898f-46de-c832-f90a8b2e59f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "3ThbvwUGBd-l"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\" #Please add your API TOken as if the GIT wont allow to upload secret keys"
      ],
      "metadata": {
        "id": "THuDqmXPJkEz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community"
      ],
      "metadata": {
        "id": "oLMKpm7GCDa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toqw9GQYBptb",
        "outputId": "64761679-ae1d-4d18-8882-3909fd532ba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.3.20\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: langchain-core, langchain-text-splitters, langsmith, pydantic, PyYAML, requests, SQLAlchemy\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-jSFUw2Bt4X",
        "outputId": "65204025-fd16-40d1-fd9f-e09df045dec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.utilities import WikipediaAPIWrapper"
      ],
      "metadata": {
        "id": "qkaXATgJBzQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wikipedia = WikipediaAPIWrapper()"
      ],
      "metadata": {
        "id": "rv021QgiB_25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wikipedia.run('Langchain')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "UCh9x8dvCbX_",
        "outputId": "77709c6e-37e6-404d-e1b6-bf066a25e31a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Page: LangChain\\nSummary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain\\'s use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\\n\\nPage: Retrieval-augmented generation\\nSummary: Retrieval-augmented generation (RAG) is a technique that enables generative artificial intelligence (Gen AI) models to retrieve and incorporate new information. It modifies interactions with a large language model (LLM) so that the model responds to user queries with reference to a specified set of documents, using this information to supplement information from its pre-existing training data. This allows LLMs to use domain-specific and/or updated information.  Use cases include providing chatbot access to internal company data or generating responses based on authoritative sources.\\nRAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce AI hallucinations, which have led to real-world issues like chatbots inventing policies or lawyers citing nonexistent legal cases.\\nBy dynamically retrieving information, RAG enables AI to provide more accurate responses without frequent retraining. According to IBM, \"RAG also reduces the need for users to continuously train the model on new data and update its parameters as circumstances evolve. In this way, RAG can lower the computational and financial costs of running LLM-powered chatbots in an enterprise setting.\"\\nBeyond efficiency gains, RAG also allows LLMs to include source references in their responses, enabling users to verify information by reviewing cited documents or original sources. This can provide greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.\\n\\n\\n\\nPage: Milvus (vector database)\\nSummary: Milvus is a distributed vector database developed by Zilliz. It is available as both open-source software and a cloud service.\\nMilvus is an open-source project under LF AI & Data Foundation distributed under the Apache License 2.0.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U langchain-openai\n"
      ],
      "metadata": {
        "id": "Dlgiv4nCCd3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI\n"
      ],
      "metadata": {
        "id": "-q6laqwXCjEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAI\n"
      ],
      "metadata": {
        "id": "0r25iXh4EGUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "3YjySXZ3EH7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import Tool\n",
        "wikipedia_tool = Tool(\n",
        "    name='wikipedia',\n",
        "    func= wikipedia.run,\n",
        "    description=\"Useful for when you need to look up a topic, country or person on wikipedia\"\n",
        ")"
      ],
      "metadata": {
        "id": "MyQdwddXESBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent\n",
        "\n",
        "# Ensure `tools` is a list\n",
        "zero_shot_agent = initialize_agent(\n",
        "    agent=\"zero-shot-react-description\",\n",
        "    tools=[wikipedia_tool],  # Wrapped in a list\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    max_iterations=3,\n",
        ")"
      ],
      "metadata": {
        "id": "rZDsLwPGEsc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_agent.run(\"When was Barak Obama born?\")"
      ],
      "metadata": {
        "id": "fyHmACxcEybI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers langchain huggingface_hub\n"
      ],
      "metadata": {
        "id": "QE3z2eqKHmiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.utilities import WikipediaAPIWrapper\n",
        "from langchain import HuggingFaceHub\n",
        "from langchain.agents import Tool, initialize_agent\n",
        "\n",
        "# Set your Hugging Face API key as an environment variable\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\"  # Replace with your actual API key\n",
        "\n",
        "# Initialize Wikipedia API Wrapper\n",
        "wikipedia = WikipediaAPIWrapper()\n",
        "\n",
        "# Define a tool for Wikipedia search\n",
        "wikipedia_tool = Tool(\n",
        "    name='wikipedia',\n",
        "    func=wikipedia.run,\n",
        "    description=\"Useful for looking up information about a topic, country, or person on Wikipedia\"\n",
        ")\n",
        "\n",
        "# Load a model from Hugging Face Hub (e.g., flan-t5-xl)\n",
        "# Explicitly specify the 'google/flan-t5-xl' model for text generation\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"google/flan-t5-xl\",  # Choose a model (change as needed)\n",
        "    model_kwargs={\"temperature\": 0.7, \"max_length\": 256},\n",
        "    task=\"text-generation\"  # Specify the task explicitly\n",
        ")\n",
        "\n",
        "# Initialize the agent\n",
        "zero_shot_agent = initialize_agent(\n",
        "    agent=\"zero-shot-react-description\",\n",
        "    tools=[wikipedia_tool],\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    max_iterations=3,\n",
        ")\n",
        "\n",
        "# Run the agent with a query\n",
        "response=zero_shot_agent.run(\"Tell me about Singapore\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "V6duX1ZHK10o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.tools import WikipediaQueryRun\n",
        "from langchain.utilities import WikipediaAPIWrapper\n",
        "from langchain import HuggingFaceHub\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "\n",
        "# Set Hugging Face API token\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\"\n",
        "\n",
        "# Initialize Wikipedia API Wrapper\n",
        "wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
        "\n",
        "# Load a model from Hugging Face Hub (flan-t5-xl)\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"google/flan-t5-xl\",\n",
        "    model_kwargs={\"temperature\": 0.7, \"max_length\": 256},\n",
        "    task=\"text2text-generation\"\n",
        ")\n",
        "\n",
        "# Initialize the agent, using AgentType.ZERO_SHOT_REACT_DESCRIPTION for single-turn interactions\n",
        "zero_shot_agent = initialize_agent(\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # Use zero-shot agent\n",
        "    tools=[wikipedia],\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    max_iterations=3,\n",
        ")\n",
        "\n",
        "# Run the agent\n",
        "response = zero_shot_agent.run(\"Tell me about Singapore\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "JPvaCZVyLI7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V4yXuDjtNIk2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMl+7iKlXOHIgN+BPzQ2rIT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}